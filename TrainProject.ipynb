{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1_ZkKSyDzrO",
        "outputId": "19dc6516-3800-4b1d-84d6-8768f36b947c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training naive_bayes...\n",
            "Naive Bayes Accuracy: 0.51\n",
            "Training logistic_regression...\n",
            "Logistic Regression Accuracy: 0.48\n",
            "Training decision_tree...\n",
            "Decision Tree Accuracy: 0.92\n",
            "Training random_forest...\n",
            "Random Forest Accuracy: 0.56\n",
            "Training knn...\n",
            "Knn Accuracy: 0.99\n",
            "\n",
            "Model Performance Summary:\n",
            "Naive Bayes         : 0.51\n",
            "Logistic Regression : 0.48\n",
            "Decision Tree       : 0.92\n",
            "Random Forest       : 0.56\n",
            "Knn                 : 0.99\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from math import log\n",
        "import random\n",
        "\n",
        "# Load and preprocess data\n",
        "def preprocess_data(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    # Convert categorical features to numerical codes\n",
        "    df['Skill'] = df['Skill'].astype('category').cat.codes\n",
        "    df['Interests'] = df['Interests'].astype('category').cat.codes\n",
        "    df['Experience_Level'] = df['Experience_Level'].map({'Beginner': 0, 'Intermediate': 1, 'Advanced': 2})\n",
        "\n",
        "    X = df.iloc[:, :-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "    return X, y\n",
        "\n",
        "# Split data\n",
        "def train_test_split(X, y, test_size=0.2, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    indices = np.arange(len(X))\n",
        "    np.random.shuffle(indices)\n",
        "    test_len = int(len(X) * test_size)\n",
        "    test_idx = indices[:test_len]\n",
        "    train_idx = indices[test_len:]\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "# Accuracy\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Naive Bayes (Updated for multiple classes)\n",
        "class MultinomialNBScratch:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.class_log_prior = {}\n",
        "        self.feature_log_prob = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.class_log_prior[c] = log(len(X_c) / len(X))\n",
        "\n",
        "            # Calculate feature probabilities for each class\n",
        "            feature_counts = np.sum(X_c, axis=0)\n",
        "            total_counts = np.sum(feature_counts)\n",
        "            self.feature_log_prob[c] = np.log((feature_counts + 1) / (total_counts + X.shape[1]))\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for x in X:\n",
        "            class_scores = {}\n",
        "            for c in self.classes:\n",
        "                # Start with the class prior\n",
        "                score = self.class_log_prior[c]\n",
        "                # Add the log probability of each feature\n",
        "                score += np.sum(x * self.feature_log_prob[c])\n",
        "                class_scores[c] = score\n",
        "            preds.append(max(class_scores, key=class_scores.get))\n",
        "        return np.array(preds)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Logistic Regression (Updated for multiple classes using One-vs-Rest)\n",
        "class LogisticRegressionScratch:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.models = []\n",
        "        self.classes = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "\n",
        "        # One-vs-Rest approach for multi-class classification\n",
        "        for c in self.classes:\n",
        "            # Create binary labels for this class\n",
        "            y_binary = np.where(y == c, 1, 0)\n",
        "\n",
        "            # Initialize weights\n",
        "            theta = np.zeros(X.shape[1])\n",
        "\n",
        "            # Train binary classifier\n",
        "            for _ in range(self.epochs):\n",
        "                z = np.dot(X, theta)\n",
        "                h = self.sigmoid(z)\n",
        "                gradient = np.dot(X.T, (h - y_binary)) / y.size\n",
        "                theta -= self.lr * gradient\n",
        "\n",
        "            self.models.append((c, theta))\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not self.models:\n",
        "            raise ValueError(\"Model not trained yet\")\n",
        "\n",
        "        # Get probabilities for each class\n",
        "        probabilities = []\n",
        "        for c, theta in self.models:\n",
        "            z = np.dot(X, theta)\n",
        "            probabilities.append(self.sigmoid(z))\n",
        "\n",
        "        # Stack probabilities and pick class with highest probability\n",
        "        prob_matrix = np.column_stack(probabilities)\n",
        "        return np.array([self.classes[i] for i in np.argmax(prob_matrix, axis=1)])\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Decision Tree (Updated for multiple classes)\n",
        "class DecisionTreeScratch:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _gini(self, y):\n",
        "        counts = Counter(y)\n",
        "        return 1 - sum((c / len(y)) ** 2 for c in counts.values())\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain = -1\n",
        "        best_feat, best_val = None, None\n",
        "        current_gini = self._gini(y)\n",
        "\n",
        "        for feature in range(X.shape[1]):\n",
        "            values = np.unique(X[:, feature])\n",
        "            for val in values:\n",
        "                left_idx = X[:, feature] <= val\n",
        "                right_idx = X[:, feature] > val\n",
        "\n",
        "                if sum(left_idx) < self.min_samples_split or sum(right_idx) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                left = y[left_idx]\n",
        "                right = y[right_idx]\n",
        "\n",
        "                if len(left) == 0 or len(right) == 0:\n",
        "                    continue\n",
        "\n",
        "                gain = current_gini - (\n",
        "                    len(left)/len(y)*self._gini(left) + len(right)/len(y)*self._gini(right))\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feat, best_val = feature, val\n",
        "\n",
        "        return best_feat, best_val\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        # Stopping conditions\n",
        "        if (depth >= self.max_depth or\n",
        "            len(set(y)) == 1 or\n",
        "            len(y) < self.min_samples_split):\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        feature, value = self._best_split(X, y)\n",
        "\n",
        "        if feature is None:  # No split improves gini\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        left_idx = X[:, feature] <= value\n",
        "        right_idx = X[:, feature] > value\n",
        "\n",
        "        left_branch = self._build_tree(X[left_idx], y[left_idx], depth + 1)\n",
        "        right_branch = self._build_tree(X[right_idx], y[right_idx], depth + 1)\n",
        "\n",
        "        return (feature, value, left_branch, right_branch)\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "\n",
        "        feature, value, left, right = node\n",
        "\n",
        "        if x[feature] <= value:\n",
        "            return self._predict_one(x, left)\n",
        "        else:\n",
        "            return self._predict_one(x, right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_one(x, self.tree) for x in X])\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Random Forest (Updated for multiple classes)\n",
        "class RandomForestScratch:\n",
        "    def __init__(self, n_estimators=10, max_depth=5, max_features=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        n_features = X.shape[1]\n",
        "        self.max_features = int(np.sqrt(n_features)) if self.max_features is None else self.max_features\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Bootstrap sample\n",
        "            idx = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[idx], y[idx]\n",
        "\n",
        "            # Random feature selection\n",
        "            feature_idx = np.random.choice(n_features, self.max_features, replace=False)\n",
        "            X_sample = X_sample[:, feature_idx]\n",
        "\n",
        "            tree = DecisionTreeScratch(max_depth=self.max_depth)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append((tree, feature_idx))\n",
        "\n",
        "    def predict(self, X):\n",
        "        all_preds = []\n",
        "        for tree, feature_idx in self.trees:\n",
        "            X_subset = X[:, feature_idx]\n",
        "            preds = tree.predict(X_subset)\n",
        "            all_preds.append(preds)\n",
        "\n",
        "        # Majority voting\n",
        "        return np.array([Counter(col).most_common(1)[0][0] for col in zip(*all_preds)])\n",
        "\n",
        "# ---------------------------------------------\n",
        "# K-Nearest Neighbors (Updated for multiple classes)\n",
        "class KNNScratch:\n",
        "    def __init__(self, k=5):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def _euclidean(self, a, b):\n",
        "        return np.sqrt(np.sum((a - b) ** 2))\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        for x in X:\n",
        "            # Calculate distances to all training points\n",
        "            distances = [self._euclidean(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "            # Get indices of k nearest neighbors\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "\n",
        "            # Get labels of nearest neighbors\n",
        "            k_labels = self.y_train[k_indices]\n",
        "\n",
        "            # Majority vote\n",
        "            preds.append(Counter(k_labels).most_common(1)[0][0])\n",
        "\n",
        "        return np.array(preds)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Save model to file\n",
        "def save_model(model, filename):\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Main driver\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    X, y = preprocess_data(\"/content/career_data_new.csv\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize models\n",
        "    models = {\n",
        "        \"naive_bayes\": MultinomialNBScratch(),\n",
        "        \"logistic_regression\": LogisticRegressionScratch(lr=0.1, epochs=1000),\n",
        "        \"decision_tree\": DecisionTreeScratch(max_depth=5),\n",
        "        \"random_forest\": RandomForestScratch(n_estimators=20, max_depth=5),\n",
        "        \"knn\": KNNScratch(k=5)\n",
        "    }\n",
        "\n",
        "    # Train and evaluate models\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        results[name] = acc\n",
        "        print(f\"{name.replace('_', ' ').title()} Accuracy: {acc:.2f}\")\n",
        "        save_model(model, f\"{name}_model.pkl\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nModel Performance Summary:\")\n",
        "    for name, acc in results.items():\n",
        "        print(f\"{name.replace('_', ' ').title():<20}: {acc:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
